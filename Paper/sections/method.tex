In order to determine if an action is performed correctly, we first need to identify the type of action. For that we first need to know what sport is playing at that moment and then search for the most likely action. Although this might be also possible to recognize action without knowing the kind of sport, however this makes the problem harder and will increase the response time. In addition we assume that when someone is playing a sport he will not change to another sport, and it should be convenient enough to ask the user to at least choose the kind of sport.

To recognize the actions we build a classifier for actions of each sport. The classifier is made with a Hidden Markov Model (HMM) for each action and recognizes a query action by comparing the likelihood of models. HMM assumes that a hidden process controls the probability of observing an event \cite{rabiner1986introduction}. The process has different hidden states and each state models a different distribution of data. In addition, the Markov property implies that the probability of being at a state at time $t$ only depends on the state of model at time $t-1$. The transitions of hidden states can have an ergodic architecture where the future state of model could be any of the $\{s_1,s_2,...,s_N\}$ states. Also we may use a left-right architecture and only allow auto-transition, and transition to the states with index greater than the current state.

In this work we use a left-right model that only allows auto-transition, and transition to the next state. This is convenient for recognizing sequences that follow a similar pattern and enforces the model to learn a continuous pattern over the action. We model the actions using HMMs with 4 hidden states. Each state models the movement features with a mixture of 2 Gaussians.

We use the data from gyroscope and accelerometer to construct the movement features. Gyroscope measures the rotation over the three axes which is a good feature for measuring the curvature of movement. The accelerometer measures the acceleration that is relative to the gravity, which is slightly different from the kinematic acceleration. An accelerometer in the rest position outputs $9.8$ on the axis that is flat to the ground, which is the gravity force. This will cause a low frequency on the axes of accelerometer that are pointed to the ground. By taking the derivative of acceleration, which is jerk, we can also filter out the low frequencies.

To recognize actions in real-time we need to consider different factors. First, the model should have a considerably low process time. Also we should be able to recognize actions without having information about the beginning and end frames of an action. Since we use a left-right architecture for the HMMs the processing time is linear and it needs $NT$ computations to identify the likelihood of an input action, where $N$ is the number of states of model and $T$ is the length of sequence. This is much faster than using an ergodic model which needs $N^2T$ processes.

We use a sliding window approach to recognize actions in real-time. With this method we segment the input data into parts with equal lengths of $32$ with $31$ frames overlap. For training the HMMs we use no overlap to avoid redundant samples. Since the data sampling rate is 100 frames per second 32 frames means about $\frac{1}{3}^{rd}$ of a second delay in output which is a good response time for our reason.

After the gesture recognition process, we need to identify the parts of an action that are not accurate. For this reason we proposed to find the similarity every frames of the action to a good sample of that action we have in the database. We use Dynamic Time Wrapping (DTW) to find the distance between the frames of a query action and the good sample of that action. With DTW we can find the closest match of a query frame on the sample in our dataset, and find the distance between the features at that point. For this analysis we use only the rotational features at this point of the research.

To identify the inaccurate parts with DTW, we need to have an entire sequence of an action. Therefore we should be able to tell an action from the transition parts between the consecutive actions and identify the beginning and ends respectively. For that we simplify the idea in \cite{lee1999hmm} and use an additional HMM to recognize non-action parts. We call this HMM the threshold model. We train the threshold HMM using all the samples of actions we have for a sport. This model should output a likelihood less than the likelihood of the action classifier for an action gesture. This is because the parameters of threshold HMM are trained on all the actions and it should be less certain than the correct model of an action. On the other hand the threshold model will output a likelihood higher than the other models on non-action sequences because of the large variance of its training data. 